{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7866EzAoXHXF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import gdown\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghu9qR4aDZyF",
        "outputId": "922296fc-521b-4ded-bc6b-0e158a959950"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM-based neural network for time series forecasting.\n",
        "\n",
        "    Parameters:\n",
        "    - input_size (int): Number of input features per time step.\n",
        "    - hidden_size (int, optional): Number of features in the hidden state. Default is 64.\n",
        "    - num_layers (int, optional): Number of stacked LSTM layers. Default is 2.\n",
        "    - bidirectional (bool, optional): If True, uses a bidirectional LSTM. Default is False.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=2, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        direction_factor = 2 if bidirectional else 1\n",
        "        self.fc = nn.Linear(hidden_size * direction_factor, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \"\"\"\n",
        "      Process:\n",
        "        1. Pass input through the LSTM layers, which returns outputs for all time steps.\n",
        "        2. Extract the output corresponding to the last time step of the sequence (out[:, -1, :]).\n",
        "        3. Pass this final output through a fully connected layer to produce the prediction.\n",
        "        4. Use squeeze() to remove any singleton dimensions, resulting in a 1D tensor.\n",
        "\n",
        "      \"\"\"\n",
        "      out, _ = self.lstm(x)\n",
        "      out = out[:, -1, :]\n",
        "      return self.fc(out).squeeze()"
      ],
      "metadata": {
        "id": "opQ_FS_Gb-ga"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Dataset wraps input and target sequnces for the DataLoader.\n",
        "  \"\"\"\n",
        "  def __init__(self, X, y):\n",
        "      self.X = torch.tensor(X, dtype=torch.float32)\n",
        "      self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Returns the number of samples in the dataset.\n",
        "    \"\"\"\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Returns the input and target sequences for a given index.\n",
        "    \"\"\"\n",
        "    return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "453X1PFla8Jm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, scaler, target_index):\n",
        "  \"\"\"\n",
        "  Evaluate the performance of the trained LSTM model on the given dataset.\n",
        "\n",
        "  Parameters:\n",
        "  1. model : nn.Module\n",
        "    The trained LSTM model.\n",
        "\n",
        "  2. dataloader : DataLoader\n",
        "    The DataLoader containing the evaluation data.\n",
        "\n",
        "  3. scaler : MinMaxScaler\n",
        "    The MinMaxScaler used to scale the data.\n",
        "\n",
        "  4. target_index (int):\n",
        "    The index of the target column in the dataset.\n",
        "\n",
        "  Returns None.\n",
        "  prints RSME MAE R^2 Score and plots the actual vs predicted values.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  predictions, actuals = [], []\n",
        "  with torch.no_grad():\n",
        "      for batch_X, batch_y in dataloader:\n",
        "          outputs = model(batch_X).unsqueeze(1)\n",
        "          predictions.append(outputs.numpy())\n",
        "          actuals.append(batch_y.numpy())\n",
        "\n",
        "  predictions = np.concatenate(predictions).reshape(-1, 1)\n",
        "  actuals = np.concatenate(actuals).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "9AcX3CgJcDKO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_create_sequences(data: pd.DataFrame, target_column: str, sequence_length: int, forecast_days: int):\n",
        "    \"\"\"\n",
        "    To preprocess the input dataframe and create sequences for time series modeling.\n",
        "\n",
        "    Parameters:\n",
        "    1. data (pd.DataFrame): dataframe containing all the features and target columns.\n",
        "    2. target_column (str): name of column to be forecasted\n",
        "    3. sequence_length (int): number of past time steps in each input sequence\n",
        "    4. forecast_days (int): number of future days to forecast (output sequence length)\n",
        "\n",
        "    Returns:\n",
        "    1. xs (np.ndarray): array of input sequences of shape (num_sequences, sequence_length, num_features)\n",
        "    2. ys (np.ndarray): array of target sequences of shape (num_sequences, forecast_days)\n",
        "    3. scaler (MinMaxScaler): MinMaxScaler object used to scale the data\n",
        "\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(scaled_data) - sequence_length - forecast_days + 1):\n",
        "        x = scaled_data[i:i+sequence_length, :]\n",
        "        y = scaled_data[i+sequence_length:i+sequence_length+forecast_days, data.columns.get_loc(target_column)]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "\n",
        "    return np.array(xs), np.array(ys), scaler"
      ],
      "metadata": {
        "id": "kmH6K38zahIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, criterion, optimizer, epochs=50):\n",
        "  \"\"\"\n",
        "  Train the LSTM model on the given dataset.\n",
        "\n",
        "  Parameters:\n",
        "  1. model : nn.Module\n",
        "    The LSTM model to be trained.\n",
        "\n",
        "  2. dataloader : DataLoader\n",
        "    The DataLoader containing the training data.\n",
        "\n",
        "  3. criterion : nn.Module\n",
        "    The loss function to be used during training.\n",
        "\n",
        "  4. optimizer : torch.optim.Optimizer\n",
        "    The optimizer to be used for updating the model's parameters.\n",
        "\n",
        "  5. epochs (int, optional):\n",
        "    The number of training epochs. Default is 50.\n",
        "\n",
        "  Returns None.\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "      epoch_loss = 0\n",
        "      for batch_X, batch_y in dataloader:\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(batch_X)\n",
        "          loss = criterion(outputs, batch_y.squeeze())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.item()\n",
        "      print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "VPgBZUFTcBDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm_model(\n",
        "    data: pd.DataFrame,\n",
        "    target_column: str,\n",
        "    sequence_length: int = 30,\n",
        "    forecast_days: int = 1,\n",
        "    hidden_size: int = 64,\n",
        "    num_layers: int = 2,\n",
        "    bidirectional: bool = False,\n",
        "    batch_size: int = 32,\n",
        "    learning_rate: float = 0.001,\n",
        "    epochs: int = 50\n",
        "):\n",
        "    # Step 1: Preprocess\n",
        "    X, y, scaler = preprocess_and_create_sequences(data, target_column, sequence_length, forecast_days)\n",
        "    X, y = X.to(device), y.to(device)  # ADD THIS CHANGE\n",
        "\n",
        "    # Step 2: Dataset & Dataloader\n",
        "    dataset = TimeSeriesDataset(X, y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Step 3: Model Initialization\n",
        "    input_size = X.shape[2]\n",
        "    model = LSTMModel(input_size, hidden_size, num_layers, bidirectional).to(device)\n",
        "    # model = model.to(device)  # ADD THIS CHANGE\n",
        "\n",
        "    # Step 4: Loss & Optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Step 5: Train\n",
        "    train_model(model, dataloader, criterion, optimizer, epochs)\n",
        "\n",
        "    return model, scaler"
      ],
      "metadata": {
        "id": "UKJJ4elZdGWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_regression_model(df, target_column,\n",
        "                           use_xgb=True, use_lgb=False, use_rf=False,\n",
        "                           feature_columns=None, split=0.8, estimators=50,\n",
        "                           random_state=42):\n",
        "    \"\"\"\n",
        "    Trains XGBoost, LightGBM, and/or Random Forest on the given DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The full dataset containing features and target.\n",
        "    target_column : str\n",
        "        The name of the target column.\n",
        "    use_xgb : bool\n",
        "        Whether to train an XGBoost model.\n",
        "    use_lgb : bool\n",
        "        Whether to train a LightGBM model.\n",
        "    use_rf : bool\n",
        "        Whether to train a Random Forest model.\n",
        "    feature_columns : list of str, optional\n",
        "        Which columns to use as features. Defaults to all columns except target.\n",
        "    split : float\n",
        "        Fraction of data to use for training (time-based split).\n",
        "    estimators : int\n",
        "        Number of estimators/trees for boosting or random forest.\n",
        "    random_state : int\n",
        "        Random seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    trained_models : dict\n",
        "        Dictionary with model names as keys and fitted model instances as values.\n",
        "    scaler : StandardScaler\n",
        "        The fitted scaler object (for scaling test data the same way).\n",
        "    \"\"\"\n",
        "\n",
        "    if split <= 0 or split >= 1:\n",
        "        raise ValueError(\"Split ratio should be between 0 and 1.\")\n",
        "\n",
        "    if sum([use_xgb, use_lgb, use_rf]) == 0:\n",
        "        raise ValueError(\"At least one model (XGBoost, LightGBM, Random Forest) must be selected.\")\n",
        "\n",
        "    if feature_columns is None:\n",
        "        feature_columns = [col for col in df.columns if col != target_column]\n",
        "\n",
        "    X = df[feature_columns]\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Time-based train/test split\n",
        "    split_index = int(len(df) * split)\n",
        "    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
        "    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "    trained_models = {}\n",
        "\n",
        "    if use_xgb:\n",
        "        xgb_model = xgb.XGBRegressor(objective='reg:squarederror',\n",
        "                                     n_estimators=estimators,\n",
        "                                     random_state=random_state)\n",
        "        xgb_model.fit(X_train_scaled, y_train)\n",
        "        trained_models['xgboost'] = xgb_model\n",
        "\n",
        "    if use_lgb:\n",
        "        lgb_model = lgb.LGBMRegressor(objective='regression',\n",
        "                                      n_estimators=estimators,\n",
        "                                      random_state=random_state)\n",
        "        lgb_model.fit(X_train_scaled, y_train)\n",
        "        trained_models['lightgbm'] = lgb_model\n",
        "\n",
        "    if use_rf:\n",
        "        rf_model = RandomForestRegressor(n_estimators=estimators,\n",
        "                                         random_state=random_state,\n",
        "                                         n_jobs=-1)\n",
        "        rf_model.fit(X_train_scaled, y_train)\n",
        "        trained_models['random_forest'] = rf_model\n",
        "\n",
        "    return trained_models, scaler\n"
      ],
      "metadata": {
        "id": "8xBJNa3mCLdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    if len(y_test) == 0 or np.all(y_test == y_test[0]):\n",
        "        return y_pred, {'MAE': np.nan, 'RMSE': np.nan, 'R^2': np.nan}\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return y_pred, {'MAE': mae, 'RMSE': rmse, 'R^2': r2}\n"
      ],
      "metadata": {
        "id": "A1WQJzWLDNlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_evaluate_boost(\n",
        "    data_df,\n",
        "    target_col,\n",
        "    trained_models,\n",
        "    scaler,\n",
        "    index_ranges,\n",
        "    context_len,\n",
        "    forecast_horizon=1,\n",
        "    step_size=1,\n",
        "    plot=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluates and optionally plots forecasts of pre-trained models over specified date ranges using a sliding window approach.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_df : pd.DataFrame\n",
        "        The full dataset containing features and the target variable.\n",
        "        The index of this DataFrame **must be a pd.DatetimeIndex** representing the dates.\n",
        "    target_col : str\n",
        "        The name of the target variable column in data_df.\n",
        "    trained_models : dict\n",
        "        A dictionary of trained models, where keys are model names (str) and values are model objects.\n",
        "        **These models must be trained beforehand using the `train_boost_model` function.**\n",
        "    scaler : sklearn.preprocessing.StandardScaler\n",
        "        The scaler fitted on the training data used to transform features before prediction.\n",
        "    index_ranges : list of tuples\n",
        "        List of tuples specifying date ranges to evaluate, e.g. [(start_date1, end_date1), (start_date2, end_date2)],\n",
        "        where each date is a pd.Timestamp or a string convertible to datetime.\n",
        "        The function converts these date ranges internally to integer index slices.\n",
        "    context_len : int\n",
        "        The number of time steps (days) to use as the historical context window for making forecasts.\n",
        "    forecast_horizon : int, optional, default=1\n",
        "        The number of future time steps (days) to predict.\n",
        "    step_size : int, optional, default=1\n",
        "        Step size to move the sliding window forward between evaluations.\n",
        "    plot : bool, optional, default=True\n",
        "        If True, plots actual vs predicted values with shaded regions indicating context and forecast periods.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    results : dict\n",
        "        A dictionary with keys as tuples (start_idx, end_idx, model_name) and values as dictionaries\n",
        "        containing lists of evaluation metrics ('MAE', 'RMSE', 'R^2') over the sliding windows.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - This function expects `data_df` to have a DatetimeIndex and uses date ranges (converted to indices)\n",
        "      to slice the data for evaluation.\n",
        "    - The models provided in `trained_models` must already be fitted; this function does not perform any training.\n",
        "    - The scaler must be consistent with the data the models were trained on.\n",
        "    - The sliding window uses the context length as the training window and then evaluates the forecast horizon.\n",
        "    - The plotting includes informative titles with date ranges, context length, and forecast horizon.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert date ranges to integer index ranges\n",
        "    index_all = data_df.index  # DatetimeIndex\n",
        "    index_ranges_int = []\n",
        "    for start_date, end_date in index_ranges:\n",
        "        start_date = pd.to_datetime(start_date)\n",
        "        end_date = pd.to_datetime(end_date)\n",
        "\n",
        "        if start_date not in index_all or end_date not in index_all:\n",
        "            raise ValueError(f\"Start or end date ({start_date}, {end_date}) not found in data_df index.\")\n",
        "\n",
        "        start_idx = index_all.get_loc(start_date)\n",
        "        end_idx = index_all.get_loc(end_date) + 1  # inclusive range, so +1 for Python slicing\n",
        "\n",
        "        index_ranges_int.append((start_idx, end_idx))\n",
        "\n",
        "    X_all = data_df.drop(columns=[target_col]).values\n",
        "    y_all = data_df[target_col].values\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for (start_idx, end_idx) in index_ranges_int:\n",
        "        print(f\"\\nEvaluating on rows {start_idx} to {end_idx} (dates {index_all[start_idx].date()} to {index_all[end_idx-1].date()}):\")\n",
        "\n",
        "        for model_name, model in trained_models.items():\n",
        "            print(f\"\\nModel: {model_name}\")\n",
        "            metrics_all = {'MAE': [], 'RMSE': [], 'R^2': []}\n",
        "\n",
        "            for i in range(start_idx, end_idx - context_len - forecast_horizon + 1, step_size):\n",
        "                train_start = i\n",
        "                train_end = i + context_len\n",
        "                test_start = train_end\n",
        "                test_end = test_start + forecast_horizon\n",
        "\n",
        "                X_test = X_all[test_start:test_end]\n",
        "                y_test = y_all[test_start:test_end]\n",
        "\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "                y_pred, metrics = evaluate(model, X_test_scaled, y_test)\n",
        "\n",
        "                for key in metrics:\n",
        "                    metrics_all[key].append(metrics[key])\n",
        "\n",
        "                if plot:\n",
        "                    plt.figure(figsize=(12, 5))\n",
        "\n",
        "                    # Convert sample indices to dates\n",
        "                    forecast_dates = index_all[test_start:test_end]\n",
        "                    context_dates = index_all[train_start:train_end]\n",
        "\n",
        "                    # Plot actual vs predicted\n",
        "                    plt.plot(forecast_dates, y_test, label='Actual', marker='o')\n",
        "                    plt.plot(forecast_dates, y_pred, label=f'{model_name} Prediction', marker='x')\n",
        "\n",
        "                    # Highlight context and forecast regions\n",
        "                    plt.axvspan(context_dates[0], context_dates[-1], color='lightblue', alpha=0.3, label='Context Range')\n",
        "                    plt.axvspan(forecast_dates[0], forecast_dates[-1], color='lightgreen', alpha=0.3, label='Forecast Horizon')\n",
        "\n",
        "                    # Title and labels\n",
        "                    plt.title(\n",
        "                        f'{model_name} Forecast\\n'\n",
        "                        f'Context Length: {context_len} | Forecast Horizon: {forecast_horizon}\\n'\n",
        "                        f'Train: {context_dates[0].date()} → {context_dates[-1].date()} | '\n",
        "                        f'Forecast: {forecast_dates[0].date()} → {forecast_dates[-1].date()}'\n",
        "                    )\n",
        "                    plt.xlabel('Date')\n",
        "                    plt.ylabel(target_col)\n",
        "                    plt.legend()\n",
        "                    plt.grid(True)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "            print(\"Average Metrics:\")\n",
        "            for key in metrics_all:\n",
        "                print(f\"{key}: {np.mean(metrics_all[key]):.4f}\")\n",
        "\n",
        "            results[(start_idx, end_idx, model_name)] = metrics_all\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "J-3WI0WJMXWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_and_plot_nn_forecast(\n",
        "    model,\n",
        "    test_loader,\n",
        "    scaler,\n",
        "    test_dates,\n",
        "    plot_start_date,\n",
        "    plot_end_date,\n",
        "    target_name=\"Target\",\n",
        "    plot=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate a trained PyTorch neural network model on test data and plot forecast results over a specified date range.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : torch.nn.Module\n",
        "        Trained PyTorch model.\n",
        "    test_loader : torch.utils.data.DataLoader\n",
        "        DataLoader yielding batches of (features, targets) for the test set.\n",
        "    scaler : sklearn.preprocessing scaler\n",
        "        Fitted scaler used to inverse transform the target values.\n",
        "    test_dates : pd.DatetimeIndex\n",
        "        The datetime index corresponding to all samples in the test set (must match order in test_loader).\n",
        "    plot_start_date : pd.Timestamp or str\n",
        "        Start date of the range to plot.\n",
        "    plot_end_date : pd.Timestamp or str\n",
        "        End date of the range to plot.\n",
        "    target_name : str, optional\n",
        "        Name of the target variable for labeling (default \"Target\").\n",
        "    plot : bool, optional\n",
        "        Whether to plot the actual vs predicted data for the date range (default True).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Dictionary with 'MAE', 'RMSE', and 'R2' metrics computed over the selected date range.\n",
        "\n",
        "    ----\n",
        "    NOTES:\n",
        "    -MAKE SURE TO INPUT A TRAINED NN Model\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            output = model(batch_X)\n",
        "            predictions.append(output.squeeze().numpy())\n",
        "            actuals.append(batch_y.numpy())\n",
        "\n",
        "    # Flatten all batches\n",
        "    predictions = np.concatenate(predictions)\n",
        "    actuals = np.concatenate(actuals)\n",
        "\n",
        "    # Inverse scale\n",
        "    pred_original = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "    actual_original = scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Select indices in test_dates within the plotting range\n",
        "    if isinstance(plot_start_date, str):\n",
        "        plot_start_date = pd.to_datetime(plot_start_date)\n",
        "    if isinstance(plot_end_date, str):\n",
        "        plot_end_date = pd.to_datetime(plot_end_date)\n",
        "\n",
        "    mask = (test_dates >= plot_start_date) & (test_dates <= plot_end_date)\n",
        "    indices = np.where(mask)[0]\n",
        "\n",
        "    if len(indices) == 0:\n",
        "        raise ValueError(\"No data points found in the specified date range.\")\n",
        "\n",
        "    y_true = actual_original[indices]\n",
        "    y_pred = pred_original[indices]\n",
        "    dates = test_dates[indices]\n",
        "\n",
        "    # Metrics\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(dates, y_true, label=\"Actual\", marker='o', color='black')\n",
        "        plt.plot(dates, y_pred, label=\"Prediction\", marker='x', color='blue')\n",
        "        plt.title(f\"{target_name} Forecast from {plot_start_date.date()} to {plot_end_date.date()}\")\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(target_name)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    print(f\"Evaluation Metrics for {plot_start_date.date()} to {plot_end_date.date()}:\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"R2 Score: {r2:.4f}\")\n",
        "\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n"
      ],
      "metadata": {
        "id": "uMvLZqd9Ga02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR PLOTTING BOOSTING MODELS\n",
        "# -> TRAIN MODEL using train_boost_model\n",
        "# -> call plot_evaluate_boost using trained model and appropriate arguments\n",
        "\n",
        "# FOR PLOTTING NN MODELS\n",
        "# -> TRAIN MODEL using train_lstm_model\n",
        "# -> call evaluate_and_plot_nn_forecast using approriate arguments"
      ],
      "metadata": {
        "id": "843X0GOCHxTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NgrGTk-OTA7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}